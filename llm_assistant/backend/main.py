"""
FastAPI åç«¯æœåŠ¡
æä¾› LLM å¯¹è¯ API
"""

import os
from typing import List, Optional
from contextlib import asynccontextmanager

import os
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
env_path = Path(__file__).parent / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"âœ… å·²åŠ è½½ .env æ–‡ä»¶: {env_path}")
else:
    print(f"âš ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

from hr_assistant import assistant, SkillsRAG


# è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    message: str
    history: Optional[List[tuple]] = []
    stream: bool = True


class ChatResponse(BaseModel):
    response: str
    skills_used: List[dict]


class TodoRequest(BaseModel):
    context: str


class TodoItem(BaseModel):
    task: str
    priority: str
    timeframe: str
    category: str


# Lifespan context manager
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    print("ğŸš€ å¯åŠ¨ HR åŠ©æ‰‹åç«¯...")
    # é¢„åŠ è½½ skills
    print(f"âœ… å·²åŠ è½½ {len(assistant.rag.skills)} ä¸ª skills")
    yield
    print("ğŸ‘‹ å…³é—­æœåŠ¡")


app = FastAPI(
    title="LLM HR åŠ©æ‰‹ API",
    description="åŸºäº Kimi API å’Œ High Output Management çš„æ™ºèƒ½ HR åŠ©æ‰‹",
    version="1.0.0",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥é™åˆ¶åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é™æ€æ–‡ä»¶æœåŠ¡ - å‰ç«¯ç•Œé¢
frontend_path = Path(__file__).parent.parent / "frontend"
if frontend_path.exists():
    app.mount("/static", StaticFiles(directory=str(frontend_path)), name="static")


@app.get("/")
async def root():
    """è¿”å›å‰ç«¯ç•Œé¢"""
    chat_html = frontend_path / "chat.html"
    if chat_html.exists():
        return FileResponse(str(chat_html))
    return {
        "message": "LLM HR åŠ©æ‰‹ API",
        "version": "1.0.0",
        "endpoints": {
            "chat": "POST /chat",
            "chat_stream": "POST /chat/stream",
            "todos": "POST /todos",
            "skills": "GET /skills",
            "health": "GET /health"
        }
    }


@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "skills_loaded": len(assistant.rag.skills)
    }


@app.get("/skills")
async def get_skills(category: Optional[str] = None, search: Optional[str] = None, limit: int = 20):
    """è·å– Skills åˆ—è¡¨"""
    skills = assistant.rag.skills
    
    if category:
        skills = [s for s in skills if s.category == category]
    
    if search:
        skills = assistant.rag.search(search, top_k=limit)
    
    return {
        "total": len(skills),
        "skills": [
            {
                "name": s.name,
                "folder": s.folder,
                "description": s.description[:200],
                "category": s.category
            }
            for s in skills[:limit]
        ]
    }


@app.get("/skills/categories")
async def get_categories():
    """è·å–æ‰€æœ‰åˆ†ç±»"""
    categories = {}
    for skill in assistant.rag.skills:
        cat = skill.category
        categories[cat] = categories.get(cat, 0) + 1
    return categories


@app.post("/chat")
async def chat(request: ChatRequest):
    """éæµå¼å¯¹è¯"""
    try:
        # è·å–ç›¸å…³ skills
        relevant_skills = assistant.rag.search(request.message, top_k=3)
        
        # è°ƒç”¨ LLM
        response_text = ""
        async for chunk in assistant.chat(
            request.message, 
            request.history, 
            stream=False
        ):
            response_text += chunk
        
        return {
            "response": response_text,
            "skills_used": [
                {
                    "name": s.name,
                    "category": s.category,
                    "description": s.description[:100]
                }
                for s in relevant_skills
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """æµå¼å¯¹è¯"""
    async def generate():
        try:
            async for chunk in assistant.chat(
                request.message, 
                request.history, 
                stream=True
            ):
                yield chunk
        except Exception as e:
            yield f"[ERROR] {str(e)}"
    
    return StreamingResponse(
        generate(),
        media_type="text/plain",
        headers={"X-Accel-Buffering": "no"}
    )


@app.post("/todos")
async def generate_todos(request: TodoRequest):
    """ç”Ÿæˆ TODO æ¸…å•"""
    todos = await assistant.get_todos(request.context)
    return {"todos": todos}


@app.get("/stats")
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    categories = {}
    for skill in assistant.rag.skills:
        cat = skill.category
        categories[cat] = categories.get(cat, 0) + 1
    
    return {
        "total_skills": len(assistant.rag.skills),
        "categories": categories,
        "api_configured": bool(os.getenv("KIMI_API_KEY") or os.getenv("DEEPSEEK_API_KEY") or os.getenv("OPENAI_API_KEY"))
    }


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
